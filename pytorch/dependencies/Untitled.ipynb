{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Sampling import Gaussian_sample\n",
    "from Model import VAE, Encoder, Decoder\n",
    "from limitedmnist import LimitedMNIST\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "h_dims = [200, 100]\n",
    "z_dim = 32\n",
    "batch_size=100\n",
    "n_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_label(batch_size, label, nlabels=2):\n",
    "    \"\"\"\n",
    "    Generates a `torch.Tensor` of size batch_size x n_labels of\n",
    "    the given label.\n",
    "\n",
    "    Example: generate_label(2, 1, 3) #=> torch.Tensor([[0, 1, 0],\n",
    "                                                       [0, 1, 0]])\n",
    "    :param batch_size: number of labels\n",
    "    :param label: label to generate\n",
    "    :param nlabels: number of total labels\n",
    "    \"\"\"\n",
    "    labels = (torch.ones(batch_size, 1) * label).type(torch.LongTensor)\n",
    "    y = torch.zeros((batch_size, nlabels))\n",
    "    y.scatter_(1, labels, 1)\n",
    "    return y.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "def onehot(k):\n",
    "    \"\"\"\n",
    "    Converts a number to its one-hot or 1-of-k representation\n",
    "    vector.\n",
    "    :param k: (int) length of vector\n",
    "    :return: onehot function\n",
    "    \"\"\"\n",
    "    def hot_vector(label):\n",
    "        y = torch.LongTensor(k)\n",
    "        y.zero_()\n",
    "        y[label] = 1\n",
    "        return y\n",
    "    return hot_vector\n",
    "\n",
    "\n",
    "def log_sum_exp(tensor, dim=None, sum_op=torch.sum):\n",
    "    \"\"\"\n",
    "    Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
    "    :param tensor: Tensor to compute LSE over\n",
    "    :param dim: dimension to perform operation over\n",
    "    :param sum_op: reductive operation to be applied, e.g. torch.sum or torch.mean\n",
    "    :return: LSE\n",
    "    \"\"\"\n",
    "    max, _ = torch.max(tensor, dim=dim, keepdim=True)\n",
    "    return torch.log(sum_op(torch.exp(tensor - max), dim=dim, keepdim=True)) + max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.arange(10)\n",
    "n = len(labels)\n",
    "\n",
    "# Load in data\n",
    "mnist_lab = LimitedMNIST('./', train=True, transform=torch.bernoulli, target_transform=onehot(n), digits=labels, fraction=0.025)\n",
    "mnist_ulab = LimitedMNIST('./', train=True, transform=torch.bernoulli, target_transform=onehot(n), digits=labels, fraction=1.0)\n",
    "mnist_val = LimitedMNIST('./', train=False, transform=torch.bernoulli, target_transform=onehot(n), digits=labels)\n",
    "\n",
    "# Unlabelled data\n",
    "unlabeled = DataLoader(mnist_ulab, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# Validation data\n",
    "validation = DataLoader(mnist_val, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# Labelled data\n",
    "labeled = DataLoader(mnist_lab, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(Classifier, self).__init__()\n",
    "        [x_dim, h_dim, y_dim] = dims\n",
    "        self.h = nn.Linear(x_dim, h_dim)\n",
    "        self.logits = nn.Linear(h_dim, y_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h(x))\n",
    "        x = F.softmax(self.logits(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DGM(VAE):\n",
    "    def __init__(self, dims, ratio):\n",
    "        self.alpha = 0.1*ratio\n",
    "        [x_dim, h_dim, z_dim, self.y_dim] = dims\n",
    "        \n",
    "        super(DGM, self).__init__([x_dim, h_dim, z_dim])\n",
    "        self.encoder = Encoder([x_dim+self.y_dim, h_dim, z_dim])\n",
    "        self.decoder = Decoder([z_dim+self.y_dim, list(reversed(h_dim)), x_dim])\n",
    "        self.classifier = Classifier([x_dim, h_dim[-1], self.y_dim])\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classifier(x)\n",
    "        if y is None:\n",
    "            return logits\n",
    "        z, z_mu, z_logvar = self.encoder(torch.cat([x,y], dim=1))\n",
    "        reconstruction = self.decoder(torch.cat([z,y], dim=1))\n",
    "        return reconstruction, logits, (z, z_mu, z_logvar)\n",
    "    \n",
    "    def sample(self, z, y):\n",
    "        y = y.type(torch.FloatTensor)\n",
    "        x = self.decoder(torch.cat([z,y], dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dgm = DGM([784, [200,100], 20, 10], 0.1)\n",
    "if cuda: dgm = dgm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(r, x):\n",
    "    epsilon=1e-7\n",
    "    return -torch.sum((x * torch.log(r + epsilon) + (1 - x) * torch.log((1 - r) + epsilon)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(logits, y):\n",
    "    return -y*torch.log(logits + 1e-8)\n",
    "def loss_function(x_reconstructed, x,  mu, logvar):\n",
    "    reconstruction_error = binary_cross_entropy(x_reconstructed, x)\n",
    "    KL_div = 0.5*(1. + logvar - mu**2 - torch.exp(logvar))\n",
    "    return reconstruction_error, torch.sum(KL_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_logger(d):\n",
    "    x, y = next(iter(validation))\n",
    "    _, y_logits = torch.max(dgm.classifier(Variable(x)), 1)\n",
    "    _, y = torch.max(y, 1)\n",
    "\n",
    "    acc = torch.sum(y_logits.data == y)/len(y)\n",
    "    d[\"Accuracy\"] = acc\n",
    "    \n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(x, y=None):\n",
    "    is_unlabeled = True if y is None else False\n",
    "    x = Variable(x)\n",
    "    if cuda:\n",
    "        x = x.cuda()\n",
    "    logits = dgm.forward(x)\n",
    "\n",
    "    loss = 0\n",
    "    if is_unlabeled==False:\n",
    "        y = Variable(y.type(torch.FloatTensor))\n",
    "        if cuda:\n",
    "            y=y.cuda()\n",
    "        x_recon, _, (z, z_mu, z_logvar) = dgm.forward(x, y)\n",
    "        reconstruction_error, KL_div = loss_function(x_recon, x, z_mu, z_logvar)\n",
    "        loss = torch.sum(reconstruction_error) - KL_div + torch.sum(dgm.alpha * -cross_entropy(logits, y))\n",
    "    \n",
    "    elif is_unlabeled:\n",
    "        for i in range(dgm.y_dim):\n",
    "            y = generate_label(batch_size, i, dgm.y_dim)\n",
    "            y = Variable(y.type(torch.FloatTensor))\n",
    "            if cuda:\n",
    "                y = y.cuda()\n",
    "            x_recon, _, (z, z_mu, z_logvar) = dgm.forward(x, y)\n",
    "            reconstruction_error, KL_div = loss_function(x_recon, x, z_mu, z_logvar)\n",
    "            loss += torch.sum(torch.mul(logits[:,i], reconstruction_error - KL_div))\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(dgm.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, unlabeled loss: 21502.297, labeled loss: 21050.078, total loss: 42552.375\n",
      "epoch: 2, unlabeled loss: 20983.039, labeled loss: 20427.412, total loss: 41410.453\n",
      "epoch: 3, unlabeled loss: 21367.160, labeled loss: 18448.693, total loss: 39815.852\n",
      "epoch: 4, unlabeled loss: 20775.436, labeled loss: 17873.164, total loss: 38648.602\n",
      "epoch: 5, unlabeled loss: 20697.492, labeled loss: 17449.678, total loss: 38147.172\n",
      "epoch: 6, unlabeled loss: 20700.348, labeled loss: 16777.822, total loss: 37478.172\n",
      "epoch: 7, unlabeled loss: 21140.793, labeled loss: 17511.396, total loss: 38652.188\n",
      "epoch: 8, unlabeled loss: 19919.182, labeled loss: 17545.756, total loss: 37464.938\n",
      "epoch: 9, unlabeled loss: 20955.654, labeled loss: 17046.859, total loss: 38002.516\n",
      "epoch: 10, unlabeled loss: 20347.039, labeled loss: 17500.830, total loss: 37847.867\n",
      "epoch: 11, unlabeled loss: 20049.412, labeled loss: 17355.887, total loss: 37405.297\n",
      "epoch: 12, unlabeled loss: 20482.496, labeled loss: 17840.299, total loss: 38322.797\n",
      "epoch: 13, unlabeled loss: 20203.660, labeled loss: 17817.486, total loss: 38021.148\n",
      "epoch: 14, unlabeled loss: 20175.385, labeled loss: 18164.004, total loss: 38339.391\n",
      "epoch: 15, unlabeled loss: 21077.623, labeled loss: 16779.393, total loss: 37857.016\n",
      "epoch: 16, unlabeled loss: 21644.471, labeled loss: 17039.195, total loss: 38683.664\n",
      "epoch: 17, unlabeled loss: 20961.359, labeled loss: 17740.561, total loss: 38701.922\n",
      "epoch: 18, unlabeled loss: 20458.662, labeled loss: 17007.025, total loss: 37465.688\n",
      "epoch: 19, unlabeled loss: 21149.193, labeled loss: 17569.461, total loss: 38718.656\n",
      "epoch: 20, unlabeled loss: 20691.506, labeled loss: 18342.459, total loss: 39033.965\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for (x, y), (u, _) in zip(cycle(labeled), unlabeled):\n",
    "        U = calculate_loss(u)\n",
    "        L = calculate_loss(x, y)\n",
    "        J = L + U\n",
    "        J.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(\"epoch: {}, unlabeled loss: {:.3f}, labeled loss: {:.3f}, total loss: {:.3f}\".format(epoch+1, U.data[0], L.data[0], J.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
